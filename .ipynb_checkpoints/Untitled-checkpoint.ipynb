{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68919218-bfde-43c9-8bf6-a4076b039534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "MAX_ACCOUNT_BALANCE = 100000\n",
    "MAX_NUM_SHARES = 1\n",
    "MAX_SHARE_PRICE = 70000\n",
    "MAX_OPEN_POSITIONS = 1\n",
    "MAX_STEPS = 20000\n",
    "MAX_BUY = 1\n",
    "\n",
    "INITIAL_ACCOUNT_BALANCE = 100\n",
    "\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.reward_range = (0, MAX_ACCOUNT_BALANCE)\n",
    "\n",
    "        # Actions of the format Buy x%, Sell x%, Hold, etc.\n",
    "        self.action_space = spaces.Box(low=np.array([-1]), high=np.array([1]), dtype=np.float16)\n",
    "\n",
    "        # Prices contains the OHCL values for the last five prices\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(6, 6), dtype=np.float16)\n",
    "\n",
    "    def _next_observation(self):\n",
    "        # Get the stock data points for the last 5 days and scale to between 0-1\n",
    "        frame = np.array([\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'Open'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'High'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'Low'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'Close'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'Volume'].values / MAX_NUM_SHARES,\n",
    "        ])\n",
    "        \n",
    "    \n",
    "        # Append additional data and scale each value to between 0-1\n",
    "        obs = np.append(frame, [[\n",
    "            self.balance / MAX_ACCOUNT_BALANCE,\n",
    "            self.max_net_worth / MAX_ACCOUNT_BALANCE,\n",
    "            self.shares_held / MAX_NUM_SHARES,\n",
    "            self.cost_basis / MAX_SHARE_PRICE,\n",
    "            self.total_shares_sold / MAX_NUM_SHARES,\n",
    "            self.total_sales_value / (MAX_NUM_SHARES * MAX_SHARE_PRICE),\n",
    "        ]], axis=0)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        # Set the current price to a random price within the time step\n",
    "        current_price =  self.df.loc[self.current_step, \"Close\"]\n",
    "\n",
    "        \n",
    "        amount = action\n",
    "\n",
    "        if action > 0:\n",
    "            # Buy amount % of balance in shares\n",
    "            total_possible = int(self.balance / current_price)\n",
    "            shares_bought = int(total_possible * amount)\n",
    "            prev_cost = self.cost_basis * self.shares_held\n",
    "            additional_cost = shares_bought * current_price\n",
    "\n",
    "            self.balance -= additional_cost\n",
    "            self.cost_basis = (\n",
    "                prev_cost + additional_cost) / (self.shares_held + shares_bought)\n",
    "            self.shares_held += shares_bought\n",
    "\n",
    "        elif action < 0:\n",
    "            # Sell amount % of shares held\n",
    "            shares_sold = int(self.shares_held * amount)\n",
    "            self.balance += shares_sold * current_price\n",
    "            self.shares_held -= shares_sold\n",
    "            self.total_shares_sold += shares_sold\n",
    "            self.total_sales_value += shares_sold * current_price\n",
    "\n",
    "        self.net_worth = self.balance + self.shares_held * current_price\n",
    "\n",
    "        if self.net_worth > self.max_net_worth:\n",
    "            self.max_net_worth = self.net_worth\n",
    "\n",
    "        if self.shares_held == 0:\n",
    "            self.cost_basis = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute one time step within the environment\n",
    "        self._take_action(action)\n",
    "\n",
    "        self.current_step += 1\n",
    "        \n",
    "        done = self.net_worth <= 0 \n",
    "        \n",
    "        if self.current_step > len(self.df.loc[:, 'Open'].values) - 6:\n",
    "            self.current_step = 0\n",
    "            done = 1\n",
    "\n",
    "        delay_modifier = (self.current_step / MAX_STEPS)\n",
    "\n",
    "        reward = self.balance * delay_modifier\n",
    "        \n",
    "\n",
    "        obs = self._next_observation()\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        self.balance = INITIAL_ACCOUNT_BALANCE\n",
    "        self.net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "        self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "        self.shares_held = 0\n",
    "        self.cost_basis = 0\n",
    "        self.total_shares_sold = 0\n",
    "        self.total_sales_value = 0\n",
    "\n",
    "        # Set the current step to a random point within the data frame\n",
    "        \n",
    "        self.current_step = random.randint(0, len(self.df.loc[:, 'Open'].values) - 100) \n",
    "\n",
    "        return self._next_observation()\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        # Render the environment to the screen\n",
    "        profit = self.net_worth - INITIAL_ACCOUNT_BALANCE\n",
    "\n",
    "        print(f'Step: {self.current_step}')\n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(\n",
    "            f'Shares held: {self.shares_held} (Total sold: {self.total_shares_sold})')\n",
    "        print(\n",
    "            f'Avg cost for held shares: {self.cost_basis} (Total sales value: {self.total_sales_value})')\n",
    "        print(\n",
    "            f'Net worth: {self.net_worth} (Max net worth: {self.max_net_worth})')\n",
    "        print(f'Profit: {profit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "403434e1-34a3-46a1-9a25-e140c74ee38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44160, 6)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./BTCUSD_2017_heure.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bf634bf-36cd-48e9-84cb-f1b1f86cfdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gregory.guichard@cdbdx.biz/.cache/pypoetry/virtualenvs/btc-rl-vMrsoe6R-py3.8/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = StockTradingEnv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ac0cea3-a0a3-4d4c-a274-b7ca9dba8286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gregory.guichard@cdbdx.biz/.cache/pypoetry/virtualenvs/btc-rl-vMrsoe6R-py3.8/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  logger.warn(\n",
      "/tmp/ipykernel_11547/1083846021.py:78: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  self.cost_basis = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 571  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1024 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 530         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004899101 |\n",
      "|    clip_fraction        | 0.0247      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -0.00112    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.05e+03    |\n",
      "|    n_updates            | 4           |\n",
      "|    policy_gradient_loss | -0.00246    |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 6.34e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: StockTradingEnv(df)])\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[1;32m     19\u001b[0m     policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m     env \u001b[38;5;241m=\u001b[39m env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     ent_coef \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m     27\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m mean_reward, std_reward \u001b[38;5;241m=\u001b[39m evaluate_policy(model, env, n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_reward=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m +/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/btc-rl-vMrsoe6R-py3.8/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:304\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    293\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    301\u001b[0m     reset_num_timesteps: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    302\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPPO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/btc-rl-vMrsoe6R-py3.8/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:270\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mrecord(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime/total_timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdump(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/btc-rl-vMrsoe6R-py3.8/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:264\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 264\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[1;32m    266\u001b[0m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/btc-rl-vMrsoe6R-py3.8/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/btc-rl-vMrsoe6R-py3.8/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import json\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./BTCUSD_2017_heure.csv')\n",
    "#df[\"Local time\"] = pd.to_datetime(df[\"Local time\"], utc=True)\n",
    "#df = df.sort_values('Local time')\n",
    "\n",
    "#from stable_baselines3.common.policies import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# The algorithms require a vectorized environment to run\n",
    "env = DummyVecEnv([lambda: StockTradingEnv(df)])\n",
    "\n",
    "model = PPO(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = env,\n",
    "    n_steps = 1024,\n",
    "    batch_size = 64,\n",
    "    n_epochs = 4,\n",
    "    gamma = 0.999,\n",
    "    gae_lambda = 0.98,\n",
    "    ent_coef = 0.01,\n",
    "    verbose=1)\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=5000)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n",
    "#model = PPO2(MlpPolicy, env, verbose=1)\n",
    "#model.learn(total_timesteps=20000)\n",
    "\n",
    "#obs = env.reset()\n",
    "#for i in range(2000):\n",
    "#    action, _states = model.predict(obs)\n",
    "#    obs, rewards, done, info = env.step(action)\n",
    "#    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f25b896-b36c-4ea6-b3ac-683a09c0db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "MAX_ACCOUNT_BALANCE = 2147483647\n",
    "MAX_NUM_SHARES = 2147483647\n",
    "MAX_SHARE_PRICE = 5000\n",
    "MAX_OPEN_POSITIONS = 5\n",
    "MAX_STEPS = 20000\n",
    "\n",
    "INITIAL_ACCOUNT_BALANCE = 10000\n",
    "\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.reward_range = (0, MAX_ACCOUNT_BALANCE)\n",
    "\n",
    "        # Actions of the format Buy x%, Sell x%, Hold, etc.\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-1]), high=np.array([1]), dtype=np.float16)\n",
    "\n",
    "        # Prices contains the OHCL values for the last five prices\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(6, 6), dtype=np.float16)\n",
    "\n",
    "    def _next_observation(self):\n",
    "        # Get the stock data points for the last 5 days and scale to between 0-1\n",
    "        frame = np.array([\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'Open'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'High'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'Low'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'Close'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'Volume'].values / MAX_NUM_SHARES,\n",
    "        ])\n",
    "\n",
    "        # Append additional data and scale each value to between 0-1\n",
    "        obs = np.append(frame, [[\n",
    "            self.balance / MAX_ACCOUNT_BALANCE,\n",
    "            self.max_net_worth / MAX_ACCOUNT_BALANCE,\n",
    "            self.shares_held / MAX_NUM_SHARES,\n",
    "            self.cost_basis / MAX_SHARE_PRICE,\n",
    "            self.total_shares_sold / MAX_NUM_SHARES,\n",
    "            self.total_sales_value / (MAX_NUM_SHARES * MAX_SHARE_PRICE),\n",
    "        ]], axis=0)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        # Set the current price to a random price within the time step\n",
    "        current_price = random.uniform(\n",
    "            self.df.loc[self.current_step, \"Open\"], self.df.loc[self.current_step, \"Close\"])\n",
    "        \n",
    "        \n",
    "        action = action[0]\n",
    "\n",
    "        if action > 0:\n",
    "            # Buy amount % of balance in shares\n",
    "            amount_to_buy = action\n",
    "            if amount_to_buy > self.balance:\n",
    "                amount_to_buy = self.balance\n",
    "                \n",
    "            self.balance += amount_to_buy\n",
    "            \n",
    "            \n",
    "            total_possible = int(self.balance / current_price)\n",
    "            shares_bought = int(total_possible * amount)\n",
    "            prev_cost = self.cost_basis * self.shares_held\n",
    "            additional_cost = shares_bought * current_price\n",
    "\n",
    "            self.balance -= additional_cost\n",
    "            self.cost_basis = (\n",
    "                prev_cost + additional_cost) / (self.shares_held + shares_bought)\n",
    "            self.shares_held += shares_bought\n",
    "\n",
    "        elif action < 0:\n",
    "            # Sell amount % of shares held\n",
    "            shares_sold = int(self.shares_held * amount)\n",
    "            self.balance += shares_sold * current_price\n",
    "            self.shares_held -= shares_sold\n",
    "            self.total_shares_sold += shares_sold\n",
    "            self.total_sales_value += shares_sold * current_price\n",
    "\n",
    "        self.net_worth = self.balance + self.shares_held * current_price\n",
    "\n",
    "        if self.net_worth > self.max_net_worth:\n",
    "            self.max_net_worth = self.net_worth\n",
    "\n",
    "        if self.shares_held == 0:\n",
    "            self.cost_basis = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute one time step within the environment\n",
    "        self._take_action(action)\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step > len(self.df.loc[:, 'Open'].values) - 6:\n",
    "            self.current_step = 0\n",
    "\n",
    "        delay_modifier = (self.current_step / MAX_STEPS)\n",
    "\n",
    "        reward = self.balance * delay_modifier\n",
    "        done = self.net_worth <= 0\n",
    "\n",
    "        obs = self._next_observation()\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        self.balance = INITIAL_ACCOUNT_BALANCE\n",
    "        self.net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "        self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "        self.shares_held = 0\n",
    "        self.cost_basis = 0\n",
    "        self.total_shares_sold = 0\n",
    "        self.total_sales_value = 0\n",
    "\n",
    "        # Set the current step to a random point within the data frame\n",
    "        self.current_step = random.randint(\n",
    "            0, len(self.df.loc[:, 'Open'].values) - 6)\n",
    "\n",
    "        return self._next_observation()\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        # Render the environment to the screen\n",
    "        profit = self.net_worth - INITIAL_ACCOUNT_BALANCE\n",
    "\n",
    "        print(f'Step: {self.current_step}')\n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(\n",
    "            f'Shares held: {self.shares_held} (Total sold: {self.total_shares_sold})')\n",
    "        print(\n",
    "            f'Avg cost for held shares: {self.cost_basis} (Total sales value: {self.total_sales_value})')\n",
    "        print(\n",
    "            f'Net worth: {self.net_worth} (Max net worth: {self.max_net_worth})')\n",
    "        print(f'Profit: {profit}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
